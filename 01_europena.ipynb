{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp europena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Europeana newspaper parsers\n",
    "\n",
    "> Parsers for Europena newspapers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this code is to create a pipeline for parsing the [Europeana newspaper bulk downloads](https://pro.europeana.eu/page/iiif#download) and converting the orignal ALTO XML formats + metadata into a format that can be ingested easily into the ü§ó [datasets](https://huggingface.co/docs/datasets/index) library and cons\n",
    "\n",
    " for #BigLAM. This code is mostly colated from other places. We used [nbdev](https://nbdev.fast.ai/) to give our code some:\n",
    "\n",
    "- basic tests\n",
    "- some basic documentation \n",
    "- make it easily instalable as a Python package. \n",
    "\n",
    "**note** some of these parsers are likely to be more generic but we'll develop them for europena newspapers for now. Once they have been tested on other collections they may be moved to a core module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import io\n",
    "import os\n",
    "import xml\n",
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from statistics import mean, stdev\n",
    "from attrs import asdict\n",
    "\n",
    "from typing import Any, Dict, Iterable, List, Optional, Union\n",
    "from attrs import define, field\n",
    "\n",
    "import xmltodict\n",
    "from toolz import partition_all\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALTO Processing\n",
    "\n",
    "ALTO is an XML format commonly used to store the outout of Opitcal Character Recogniton software. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test data\n",
    "\n",
    "A small amount of test data is included in the repository and versioned with Git. To avoid bloating the repository too much the below cell can either be run as part of test (including) slow tests or run locally to give oneself a bigger amount of data to test methods with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # |slow\n",
    "# !mkdir test_data\n",
    "# !aria2c -x 4 -d test_data/ ftp://download.europeana.eu/newspapers/fulltext/alto/9200396.zip\n",
    "# !unzip test_data/*.zip -d test_data/\n",
    "# !rm test_data/*.zip\n",
    "# !mkdir test_data/metadata\n",
    "# !aria2c -x 4 -d test_data/metadata/ ftp://download.europeana.eu/newspapers/metadata/9200396.zip\n",
    "# !unzip test_data/metadata/*.zip -d test_data/metadata/\n",
    "# !rm test_data/metadata/*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106562"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alto_xmls = [f for f in Path(\"test_data\").rglob(\"*.xml\") if \"edm\" not in f.name]\n",
    "len(alto_xmls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse ALTO XMLs\n",
    "\n",
    "The first step is to parse the xml file from disk into a elementree that we can use for other takss\n",
    "stolen from; https://github.com/cneud/alto-tools/blob/master/alto_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def alto_parse(alto: Union[str, Path], **kwargs):\n",
    "    \"\"\"Convert ALTO xml file to element tree\"\"\"\n",
    "    try:\n",
    "        xml = ET.parse(alto, **kwargs)\n",
    "    except ET.ParseError as e:\n",
    "        logger.error(f\"Parser Error in file '{alto}': {e}\")\n",
    "        return None\n",
    "    # Register ALTO namespaces\n",
    "    # https://www.loc.gov/standards/alto/ | https://github.com/altoxml\n",
    "    # alto-bnf (unoffical) BnF ALTO dialect - for further info see\n",
    "    # http://bibnum.bnf.fr/alto_prod/documentation/alto_prod.html\n",
    "    namespace = {\n",
    "        \"alto-1\": \"http://schema.ccs-gmbh.com/ALTO\",\n",
    "        \"alto-2\": \"http://www.loc.gov/standards/alto/ns-v2#\",\n",
    "        \"alto-3\": \"http://www.loc.gov/standards/alto/ns-v3#\",\n",
    "        \"alto-4\": \"http://www.loc.gov/standards/alto/ns-v4#\",\n",
    "        \"alto-5\": \"http://schema.ccs-gmbh.com/docworks/version20/alto-1-4.xsd\",\n",
    "        \"alto-bnf\": \"http://bibnum.bnf.fr/ns/alto_prod\",\n",
    "    }\n",
    "    # Extract namespace from document root\n",
    "    if \"http://\" in str(xml.getroot().tag.split(\"}\")[0].strip(\"{\")):\n",
    "        xmlns = xml.getroot().tag.split(\"}\")[0].strip(\"{\")\n",
    "    else:\n",
    "        try:\n",
    "            ns = xml.getroot().attrib\n",
    "            xmlns = str(ns).split(\" \")[1].strip(\"}\").strip(\"'\")\n",
    "        except IndexError:\n",
    "            logger.warning(f\"File {alto.name}: no namespace declaration found.\")\n",
    "            xmlns = \"no_namespace_found\"\n",
    "    if xmlns in namespace.values():\n",
    "        return alto, xml, xmlns\n",
    "    else:\n",
    "        logger.warning(f\"File {alto.name}: namespace {xmlns} is not registered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('test_data/9200396/BibliographicResource_3000118436002/88.xml')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alto_xmls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, xml, ns = alto_parse(alto_xmls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 12:11:06.762 | ERROR    | __main__:alto_parse:7 - Parser Error in file 'fake.xml': no element found: line 1, column 0\n"
     ]
    }
   ],
   "source": [
    "assert all([fname, xml, ns])\n",
    "Path(\"fake.xml\").touch(exist_ok=True)\n",
    "bad_xml = alto_parse(Path(\"fake.xml\"))\n",
    "# assert isinstance(bad_xml, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_alto_text(xml, xmlns, join_lines=True):\n",
    "    \"\"\"Extract text content from ALTO xml file\"\"\"\n",
    "    all_text = []\n",
    "    all_wc = []\n",
    "    # Find all <TextLine> elements\n",
    "    for lines in xml.iterfind(\".//{%s}TextLine\" % xmlns):\n",
    "        # Find all <String> elements\n",
    "        for line in lines.findall(\"{%s}String\" % xmlns):\n",
    "            wc = line.attrib[\"WC\"]\n",
    "            if wc is not None:\n",
    "                all_wc.append(float(wc))\n",
    "            # Check if there are no hyphenated words\n",
    "            if \"SUBS_CONTENT\" not in line.attrib and \"SUBS_TYPE\" not in line.attrib:\n",
    "                # Get value of attribute @CONTENT from all <String> elements\n",
    "                text = line.attrib.get(\"CONTENT\")  # + ' '\n",
    "            elif \"HypPart1\" in line.attrib.get(\"SUBS_TYPE\"):\n",
    "                text = line.attrib.get(\"SUBS_CONTENT\")  # + ' '\n",
    "                if \"HypPart2\" in line.attrib.get(\"SUBS_TYPE\"):\n",
    "                    pass\n",
    "            all_text.append(text)\n",
    "    if all_wc:\n",
    "        mean_ocr = mean(all_wc)\n",
    "    if len(all_wc) > 2:\n",
    "        std_ocr = stdev(all_wc)\n",
    "    else:\n",
    "        mean_ocr = None\n",
    "        std_ocr = None\n",
    "    return \" \".join(all_text), mean_ocr, std_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, wc, std_ocr = get_alto_text(xml, ns)\n",
    "assert all([text, wc, std_ocr])\n",
    "assert isinstance(text, str)\n",
    "assert isinstance(wc, float)\n",
    "assert isinstance(std_ocr, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def alto_illustrations(xml, xmlns):\n",
    "    \"\"\"Extract bounding boxes of illustration from ALTO xml file\"\"\"\n",
    "    # Find all <Illustration> elements\n",
    "    bounding_boxes = []\n",
    "    for illustration in xml.iterfind(\".//{%s}Illustration\" % xmlns):\n",
    "        # Get @ID of <Illustration> element\n",
    "        illustration_id = illustration.attrib.get(\"ID\")\n",
    "        # Get coordinates of <Illustration> element\n",
    "        illustration_coords = list(\n",
    "            map(\n",
    "                float,\n",
    "                (\n",
    "                    illustration.attrib.get(\"HEIGHT\"),\n",
    "                    illustration.attrib.get(\"WIDTH\"),\n",
    "                    illustration.attrib.get(\"VPOS\"),\n",
    "                    illustration.attrib.get(\"HPOS\"),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        bounding_boxes.append(illustration_coords)\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alto_illustrations(xml, ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_illustrations(xmls):\n",
    "    for file in xmls:\n",
    "        with open(file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"Illustration\" in line:\n",
    "                    yield file\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustration_xmls = list(take(10, get_illustrations(alto_xmls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('test_data/9200396/BibliographicResource_3000118436002/1.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118435687/76.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118435687/3.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118435687/1.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118436230/76.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118436230/9.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118436230/20.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118436230/33.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118436230/32.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118436230/1.xml')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "illustration_xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in illustration_xmls:\n",
    "    fname, xml, ns = alto_parse(file)\n",
    "    bounding_boxes = alto_illustrations(xml, ns)\n",
    "    assert bounding_boxes\n",
    "    for box in bounding_boxes:\n",
    "        assert isinstance(box, list)\n",
    "        assert len(box) == 4\n",
    "        assert all(isinstance(x, float) for x in box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newspaper page container \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@define(slots=True)\n",
    "class NewspaperPageAlto:\n",
    "    fname: Union[str, Path]\n",
    "    text: Optional[str]\n",
    "    mean_ocr: Optional[float]\n",
    "    std_ocr: Optional[float]\n",
    "    bounding_boxes: List[Union[float, None]]\n",
    "    item_id: str = field(init=False)\n",
    "\n",
    "    def _get_id(self):\n",
    "        return \"/\".join(Path(self.fname).parts[-3:-1])\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.item_id = self._get_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def parse_newspaper_page(xml_fname: Union[str, Path]):\n",
    "    fname, xml, ns = alto_parse(xml_fname)\n",
    "    text, wc, std_ocr = get_alto_text(xml, ns)\n",
    "    bounding_boxes = alto_illustrations(xml, ns)\n",
    "    return NewspaperPageAlto(xml_fname, text, wc, std_ocr, bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = parse_newspaper_page(alto_xmls[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert page\n",
    "assert isinstance(page, NewspaperPageAlto)\n",
    "assert isinstance(page.text, (str, None))\n",
    "assert isinstance(page.mean_ocr, (float, None))\n",
    "assert isinstance(page.std_ocr, (float, None))\n",
    "assert isinstance(page.bounding_boxes, List)\n",
    "assert isinstance(page.item_id, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewspaperPageAlto(fname=Path('test_data/9200396/BibliographicResource_3000118436002/9.xml'), text=\"√Øg. H√©ctmlre . SSS du thr√¥ne & de l√©giflation dans le ro√Øaume, l‚Äôextin√¢ion des guerres entre le Souverain & les vaffaux , entre les provinces & les provinces provinces ; qu‚Äôainfi fans entrer dans les motifs √† ne juger que par l‚Äô√©v√©nementles croifa- des ont √©pargn√© √† la nation plus de fang qu‚Äôelles n‚Äôen ont fait r√©pandre, qu‚Äôelles ont pof√© la bafe & les fondemens de la tranquillit√© tranquillit√© int√©rieure de l‚Äô√©tat. Je ne vous ferai pas remarquer que l‚Äôentreprife de Louis n‚Äôoffroit point d‚Äôobftacles capables de ralentir fort ^ .courage & d‚Äôalarmer fa fagefl'e; qu‚Äôon a voit v√ª Godefroy de Bouillon , avec les d√©bris d‚Äôune arm√©e, amas fortuit de diverfes nations, nations, √©lever dans J√©rufalem un thr√¥ne qui, vainqueur des Mufulmans , n‚Äôa p√©ri que par Autres les di vidons & les perfidies des chr√©tiens, je r√©fl. fur les n‚Äôajouterai point que Chypre, Conftantino- croifadesii pie, fournis √† des princes de la communion A Q Q a l√Ço√ª√ä latine, donnoit √† Louis des facilit√©s que n‚Äôeut , 77 g, p .. point Godefroid de Bouillon ‚Äû. ‚Äî 15. D√©c, Le parall√®le de Bajazet & de St, Louis, P- qu‚Äôon lit dans le pan√©gyrique de ce Saint, eft un tableau dont tous les traits concou rent au triomphe de la religion du faint Roi. Les . pan√©gyriftes des h√©ros turcs & Chinois, ( & i! y en a aujourd‚Äôhui un bien grand nombre ) font pri√©s d‚Äôen faire un qui puiffe l√©rvir de pendant √† celui-ci. ‚Äú Que le fe&ateur de Mahomet d√©ploie maintenant fes faites, il nous montrera l‚ÄôAile , l‚ÄôAfrique, l‚ÄôAfrique, l‚ÄôEurope englouties par fes conqu√©rons , & le thr√¥ne de fes Califes √©lev√© fur les ruines fie l‚Äôapivers j il nous piontrera donc ce courage courage\", mean_ocr=0.5861000468510345, std_ocr=0.1977007715324908, bounding_boxes=[], item_id='9200396/BibliographicResource_3000118436002')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get metadata \n",
    "\n",
    "The next step is to create some functionality to get metadata for the items. There are two possible ways we can do this:\n",
    "- via the metadata download dumps\n",
    "- via the Europena API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_examples = list(take(500, Path(\"test_data\").rglob(\"*edm.xml\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@define(slots=True)\n",
    "class NewspaperPageMetadata:\n",
    "    metadata_xml_fname: Union[str, Path]\n",
    "    title: Optional[str]\n",
    "    issued: Optional[str]\n",
    "    language: Union[List[str], str, None]\n",
    "    item_iiif_url: Optional[str]\n",
    "    all_metadata_dict: Dict[Any, Any]\n",
    "    issued_parsed: datetime = field(init=False)\n",
    "\n",
    "    def _parse_date(self, issued):\n",
    "        try:\n",
    "            date = datetime.strptime(issued, \"%Y-%m-%d\")\n",
    "            return date\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.language = (\n",
    "            self.language.split(\",\")\n",
    "            if isinstance(self.language, str)\n",
    "            else self.language\n",
    "        )\n",
    "\n",
    "        self.title = self.title.split(\"-\")[0].strip(\" \")\n",
    "        self.metadata_xml_fname = str(self.metadata_xml_fname)\n",
    "        self.issued_parsed = self._parse_date(self.issued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_metadata_from_xml(xml_file: Union[Path, str]):\n",
    "    with open(xml_file, \"r\") as f:\n",
    "        xml = xmltodict.parse(f.read())\n",
    "    metadata = xml[\"rdf:RDF\"]\n",
    "    ProvidedCHO = metadata[\"edm:ProvidedCHO\"]\n",
    "    title = ProvidedCHO[\"dc:title\"]\n",
    "    issued = ProvidedCHO[\"dcterms:issued\"]\n",
    "    language = ProvidedCHO[\"dc:language\"]\n",
    "    iiif_url = metadata[\"ore:Aggregation\"][\"edm:isShownBy\"][\"@rdf:resource\"]\n",
    "    return NewspaperPageMetadata(xml_file, title, issued, language, iiif_url, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = metadata_examples[-23]\n",
    "with open(xml_file, \"r\") as f:\n",
    "    xml = xmltodict.parse(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metadata_xml in metadata_examples:\n",
    "    metadata = get_metadata_from_xml(metadata_xml)\n",
    "    assert metadata\n",
    "    assert isinstance(metadata.language, (list, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking metadata and XML\n",
    "We need to be able to link from the data we got from the ALTO XML with our metadata and smush them together. We have our `page.item_id` attribute which will hopefully be sufficient to grab the related metadata file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1783, 6, 1, 0, 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.issued_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_metadata_for_page(\n",
    "    page: NewspaperPageAlto, metadata_directory: Optional[str] = None\n",
    "):\n",
    "    short_id = page.item_id.split(\"_\")[-1]\n",
    "    metadata_xml = f\"{metadata_directory}/http%3A%2F%2Fdata.theeuropeanlibrary.org%2FBibliographicResource%2F{short_id}.edm.xml\"\n",
    "    return get_metadata_from_xml(metadata_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_metadata_for_page(page, metadata_directory=\"test_data/metadata\")\n",
    "assert metadata\n",
    "assert isinstance(metadata, NewspaperPageMetadata)\n",
    "assert page.item_id.split(\"_\")[-1] in metadata.metadata_xml_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@define(slots=True)\n",
    "class NewspaperPage:\n",
    "    fname: Union[str, Path]\n",
    "    text: Optional[str]\n",
    "    mean_ocr: Optional[float]\n",
    "    std_ocr: Optional[float]\n",
    "    bounding_boxes: List[Union[float, None]]\n",
    "    item_id: str\n",
    "    metadata_xml_fname: Union[str, Path]\n",
    "    title: Optional[str]\n",
    "    issued: Optional[str]\n",
    "    issued_parsed: Optional[datetime]\n",
    "    language: Union[List[str], None]\n",
    "    item_iiif_url: Optional[str]\n",
    "    # all_metadata_dict: Dict[Any, Any]\n",
    "    multi_language: bool = field(init=False)\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.fname = str(self.fname)\n",
    "        self.metadata_xml_fname = str(self.metadata_xml_fname)\n",
    "        self.language = (\n",
    "            [lang for lang in self.language if lang != \"==\"]\n",
    "            if isinstance(self.language, list)\n",
    "            else self.language\n",
    "        )\n",
    "        self.multi_language = isinstance(self.language, list) and len(self.language) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def process_newspaper_page(\n",
    "    xml_file: Union[str, Path], metadata_directory: Optional[str] = None\n",
    ") -> Dict[Any, Any]:\n",
    "    page = parse_newspaper_page(xml_file)\n",
    "    metadata = get_metadata_for_page(page, metadata_directory=metadata_directory)\n",
    "    metadata = asdict(metadata)\n",
    "    metadata.pop(\"all_metadata_dict\")\n",
    "    page = asdict(page)\n",
    "    return NewspaperPage(**page, **metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    process_newspaper_page(xml, metadata_directory=\"test_data/metadata\")\n",
    "    for xml in alto_xmls[:32]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewspaperPage(fname='test_data/9200396/BibliographicResource_3000118436002/88.xml', text='Idem , par Mr***. en lui prifentent un exemplaire de fes fermons imprim√©s. 15. D√©cembre. D√©cembre. Voix {_ la), du vi ni patriote catholique, oppof√©e √† celle des faux patriotes toltrans. 1. Septembre. 1 1 ‚ñ†ladre parmi les Ombres, avec cette √©pigraphe: etgo erruvtmus & errare fecimus. 1. Novembre. . . 332 Voltaire de retour des Ombres , & fur le point d‚Äôy retourner pour n‚Äôen plus revenir l. Novembre. Page 57o 333 ‚ÄôOn fait favoir que les biens des ci devant J√©fuites de la ni√©tai√Æie √† Kockellchcur √† ¬´ne lieue de cette ville, confftans ea maifocs, terres, prairies , √©tangs & bois , partag√©s en dix-neuf lots , font √† vendre , chaque lot f√©par√©- ment ; que les amateurs d‚Äôun ou plul√Æeurs lots pourront s‚Äôadrefler au Receveur des Domaines Luxembourg Leonardy pendant le mois de Janvier prochain , & faire leurs offres , apr√®s lequel mois √©coul√© l‚Äôon ne recevera plus aucune offie i & que le 19e. de ce mois de D√©cembre, a s heures du matin & jours fuivans, le dit Receveur vendra au dit Kockclfcheur tous les dirvaux foin, paille, &. les meubles de ladite m√©tairie .au plus offrant & dernier ench√©rilleur, argent comptant. Luxembourg le ; D√©cembre 1773. CD b√™tes √† cornes & autres beftianx >', mean_ocr=0.44245891427834905, std_ocr=0.18921927369326655, bounding_boxes=[], item_id='9200396/BibliographicResource_3000118436002', metadata_xml_fname='test_data/metadata/http%3A%2F%2Fdata.theeuropeanlibrary.org%2FBibliographicResource%2F3000118436002.edm.xml', title='Journal historique et litt√©raire', issued='1776-12-15', issued_parsed=datetime.datetime(1776, 12, 15, 0, 0), language=['fr'], item_iiif_url='https://iiif.europeana.eu/image/2TS6TSUK5ULAT2TQMYN7UGBDCPKQBLHQPTPDD6GGYB4QOZXR72EQ/presentation_images/bc994340-0232-11e6-a696-fa163e2dd531/node-3/image/BNL/Journal_historique_et_litt√©raire/1776/12/15/00547/full/full/0/default.jpg', multi_language=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_newspaper_page(alto_xmls[0], metadata_directory=\"test_data/metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewspaperPage(fname='test_data/9200396/BibliographicResource_3000118436230/38.xml', text=\"Journal h√Æfl. & litt . licjri , r√©gent de Lithuanie, ‚Äî‚Äî regimens d‚Äôinfanterie, qui font au nombre de 38 , doivent √™tre port√©s √† j 500 hommes, hommes, au lieu de 1200, dont ils √©toient Compofcs ci'- devant ; l√©s pontonniers qui √©toient de 608 hommes, en formeront 1000, ainfi que le corps d‚Äôartillerie ; pour c√ß dernier on n‚Äôadmet que des gens inftruits & qui ont achev√© leurs √©tudes. La cavalerie- nationale s‚Äôaccro√Æt de jour en jour. Chaque r√©giment d‚ÄôOulans fera augment√© de 290 hommes. hommes. 'On entend par-tout les tambours annoncer annoncer la recrue, depuis environ 4 femaines, il nous eft arrive au-del√† de 5000 des plus beaux hommes, la plupart d√©ferteurs allemands. Jufqu‚Äôici nous' ignorons abfolument l‚Äôobjet d‚Äôune lev√©e fi nombrenfe & fi. extraordinaire. Dantzig (/e 3 Janvier. ) L‚Äôafl√®mbl√©e des trois Ordres de la ville , convoqu√©e le c8 D√©cembre, en conf√©quence des ordres venus de Varfovie pour figner la convention relative relative √† la navigation de la Viftule, fut prorog√©e prorog√©e au lendemain ; & c‚Äôeft ce jour-l√† que les trois Ordres ont donn√© leur confentement unanime pour la fignature de cette convention. convention. Le magiftrat a fait informer de cette, r√©folution M r . de Pfennig, commiffaire de Pologne, M r . de P√©terfon , r√©fident de Ruf- fie, & M r . de Lindenowski, r√©fident de Pruffe, avec priere d‚Äôen rendre compte √† leurs cours refpe√¢ives. Cependant le peuple en g√©n√©ral g√©n√©ral n‚Äôeft pas content, & on craint que dans l‚Äôoccafion il n‚Äôcclate en murmures & peut- %re en tumultes f√©ditieux. Tous nos\", mean_ocr=0.5256414447269231, std_ocr=0.19060038635865742, bounding_boxes=[], item_id='9200396/BibliographicResource_3000118436230', metadata_xml_fname='test_data/metadata/http%3A%2F%2Fdata.theeuropeanlibrary.org%2FBibliographicResource%2F3000118436230.edm.xml', title='Journal historique et litt√©raire', issued='1785-02-01', issued_parsed=datetime.datetime(1785, 2, 1, 0, 0), language=['fr'], item_iiif_url='https://iiif.europeana.eu/image/NGJNELURKNAPK5EL35D32X3AV36I7Z2X67NW4OGTN3EEO7L5N6NA/presentation_images/b171fd00-0222-11e6-a696-fa163e2dd531/node-3/image/BNL/Journal_historique_et_litt√©raire/1785/02/01/00157/full/full/0/default.jpg', multi_language=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = process_newspaper_page(alto_xmls[200], metadata_directory=\"test_data/metadata\")\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from datasets import Dataset\n",
    "from datasets import Value, Sequence, Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "features = Features(\n",
    "    {\n",
    "        \"fname\": Value(dtype=\"string\", id=None),\n",
    "        \"text\": Value(dtype=\"string\", id=None),\n",
    "        \"mean_ocr\": Value(dtype=\"float64\", id=None),\n",
    "        \"std_ocr\": Value(dtype=\"float64\", id=None),\n",
    "        \"bounding_boxes\": Sequence(\n",
    "            feature=Sequence(\n",
    "                feature=Value(dtype=\"float64\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            length=-1,\n",
    "            id=None,\n",
    "        ),\n",
    "        \"item_id\": Value(dtype=\"string\", id=None),\n",
    "        \"metadata_xml_fname\": Value(dtype=\"string\", id=None),\n",
    "        \"title\": Value(dtype=\"string\", id=None),\n",
    "        \"issued\": Value(dtype=\"string\", id=None),\n",
    "        \"issued_parsed\": Value(dtype=\"date64\", id=None),\n",
    "        \"language\": Sequence(\n",
    "            feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "        ),\n",
    "        \"item_iiif_url\": Value(dtype=\"string\", id=None),\n",
    "        \"multi_language\": Value(dtype=\"bool\", id=None),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@logger.catch()\n",
    "def process_batch(xml_batch: Iterable[Union[str, Path]], metadata_directory=None):\n",
    "    batch = [\n",
    "        asdict(process_newspaper_page(xml, metadata_directory=metadata_directory))\n",
    "        for xml in xml_batch\n",
    "    ]\n",
    "\n",
    "    batch = {key: [i[key] for i in batch] for key in batch[0]}\n",
    "\n",
    "    return Dataset.from_dict(batch, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = process_batch(alto_xmls[:32], metadata_directory=\"test_data/metadata\")\n",
    "assert len(ds) == 32\n",
    "assert len(ds.column_names) == 13\n",
    "assert ds.column_names == [\n",
    "    \"fname\",\n",
    "    \"text\",\n",
    "    \"mean_ocr\",\n",
    "    \"std_ocr\",\n",
    "    \"bounding_boxes\",\n",
    "    \"item_id\",\n",
    "    \"metadata_xml_fname\",\n",
    "    \"title\",\n",
    "    \"issued\",\n",
    "    \"issued_parsed\",\n",
    "    \"language\",\n",
    "    \"item_iiif_url\",\n",
    "    \"multi_language\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fname', 'text', 'mean_ocr', 'std_ocr', 'bounding_boxes', 'item_id', 'metadata_xml_fname', 'title', 'issued', 'issued_parsed', 'language', 'item_iiif_url', 'multi_language'],\n",
       "    num_rows: 32\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fname': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'mean_ocr': Value(dtype='float64', id=None),\n",
       " 'std_ocr': Value(dtype='float64', id=None),\n",
       " 'bounding_boxes': Sequence(feature=Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'item_id': Value(dtype='string', id=None),\n",
       " 'metadata_xml_fname': Value(dtype='string', id=None),\n",
       " 'title': Value(dtype='string', id=None),\n",
       " 'issued': Value(dtype='string', id=None),\n",
       " 'issued_parsed': Value(dtype='date64', id=None),\n",
       " 'language': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'item_iiif_url': Value(dtype='string', id=None),\n",
       " 'multi_language': Value(dtype='bool', id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "def process(\n",
    "    xml_files: Iterable[Union[str, Path]],\n",
    "    batch_size: int = 32,\n",
    "    metadata_directory: Optional[Union[str, Path]] = None,\n",
    "    max_workers: Optional[int] = None,\n",
    "):\n",
    "    with tqdm(total=len(xml_files) // batch_size) as pbar:\n",
    "        if not max_workers:\n",
    "            max_workers = multiprocessing.cpu_count()\n",
    "        futures = []\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for batch in partition_all(batch_size, xml_files):\n",
    "                batch = list(batch)\n",
    "                future = executor.submit(\n",
    "                    process_batch, batch, metadata_directory=metadata_directory\n",
    "                )\n",
    "                future.add_done_callback(lambda p: pbar.update(1))\n",
    "                futures.append(future)\n",
    "    return [future.result() for future in as_completed(futures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d85da29ae2d43c580b96a73ae95c20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = process(alto_xmls[:10], metadata_directory=\"test_data/metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fname', 'text', 'mean_ocr', 'std_ocr', 'bounding_boxes', 'item_id', 'metadata_xml_fname', 'title', 'issued', 'issued_parsed', 'language', 'item_iiif_url', 'multi_language'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = concatenate_datasets(datasets)\n",
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
