{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp europena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Europeana newspaper parsers\n",
    "\n",
    "> Parsers for Europena newspapers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this code is to create a pipeline for parsing the [Europeana newspaper bulk downloads](https://pro.europeana.eu/page/iiif#download) and converting the orignal ALTO XML formats + metadata into a format that can be ingested easily into the ðŸ¤— [datasets](https://huggingface.co/docs/datasets/index) library and cons\n",
    "\n",
    " for #BigLAM. This code is mostly colated from other places. We used [nbdev](https://nbdev.fast.ai/) to give our code some:\n",
    "\n",
    "- basic tests\n",
    "- some basic documentation \n",
    "- make it easily instalable as a Python package. \n",
    "\n",
    "**note** some of these parsers are likely to be more generic but we'll develop them for europena newspapers for now. Once they have been tested on other collections they may be moved to a core module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import io\n",
    "import os\n",
    "import xml\n",
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "# from dataclaises import asdict, dataclass, field\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from statistics import mean, stdev\n",
    "from attrs import asdict\n",
    "\n",
    "from typing import Any, Dict, Iterable, List, Optional, Union\n",
    "from attrs import define, field\n",
    "\n",
    "import xmltodict\n",
    "from toolz import partition_all\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALTO Processing\n",
    "\n",
    "ALTO is an XML format commonly used to store the outout of Opitcal Character Recogniton software. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test data\n",
    "\n",
    "A subset of data is included in the GitHub repo for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # |slow\n",
    "# !mkdir test_data\n",
    "# !aria2c -x 4 -d test_data/ ftp://download.europeana.eu/newspapers/fulltext/alto/9200396.zip\n",
    "# !unzip test_data/*.zip -d test_data/\n",
    "# !rm test_data/*.zip\n",
    "# !mkdir test_data/metadata\n",
    "# !aria2c -x 4 -d test_data/metadata/ ftp://download.europeana.eu/newspapers/metadata/9200396.zip\n",
    "# !unzip test_data/metadata/*.zip -d test_data/metadata/\n",
    "# !rm test_data/metadata/*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a list of ALTO XML files we can use for testing as we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alto_xmls = [f for f in Path(\"test_data\").rglob(\"*.xml\") if \"edm\" not in f.name]\n",
    "len(alto_xmls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse ALTO XMLs\n",
    "\n",
    "The first step is to parse the xml file from disk into a elementree that we can use for other takss\n",
    "stolen from; https://github.com/cneud/alto-tools/blob/master/alto_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def alto_parse(alto: Union[str, Path], **kwargs):\n",
    "    \"\"\"Convert ALTO xml file to element tree\"\"\"\n",
    "    try:\n",
    "        xml = ET.parse(alto, **kwargs)\n",
    "    except ET.ParseError as e:\n",
    "        logger.error(f\"Parser Error in file '{alto}': {e}\")\n",
    "        return None\n",
    "    # Register ALTO namespaces\n",
    "    # https://www.loc.gov/standards/alto/ | https://github.com/altoxml\n",
    "    # alto-bnf (unoffical) BnF ALTO dialect - for further info see\n",
    "    # http://bibnum.bnf.fr/alto_prod/documentation/alto_prod.html\n",
    "    namespace = {\n",
    "        \"alto-1\": \"http://schema.ccs-gmbh.com/ALTO\",\n",
    "        \"alto-2\": \"http://www.loc.gov/standards/alto/ns-v2#\",\n",
    "        \"alto-3\": \"http://www.loc.gov/standards/alto/ns-v3#\",\n",
    "        \"alto-4\": \"http://www.loc.gov/standards/alto/ns-v4#\",\n",
    "        \"alto-5\": \"http://schema.ccs-gmbh.com/docworks/version20/alto-1-4.xsd\",\n",
    "        \"alto-bnf\": \"http://bibnum.bnf.fr/ns/alto_prod\",\n",
    "    }\n",
    "    # Extract namespace from document root\n",
    "    if \"http://\" in str(xml.getroot().tag.split(\"}\")[0].strip(\"{\")):\n",
    "        xmlns = xml.getroot().tag.split(\"}\")[0].strip(\"{\")\n",
    "    else:\n",
    "        try:\n",
    "            ns = xml.getroot().attrib\n",
    "            xmlns = str(ns).split(\" \")[1].strip(\"}\").strip(\"'\")\n",
    "        except IndexError:\n",
    "            logger.warning(f\"File {alto.name}: no namespace declaration found.\")\n",
    "            xmlns = \"no_namespace_found\"\n",
    "    if xmlns in namespace.values():\n",
    "        return alto, xml, xmlns\n",
    "    else:\n",
    "        logger.warning(f\"File {alto.name}: namespace {xmlns} is not registered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alto_xmls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, xml, ns = alto_parse(alto_xmls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([fname, xml, ns])\n",
    "Path(\"fake.xml\").touch(exist_ok=True)\n",
    "bad_xml = alto_parse(Path(\"fake.xml\"))\n",
    "# assert isinstance(bad_xml, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_alto_text(xml, xmlns, join_lines=True):\n",
    "    \"\"\"Extract text content from ALTO xml file\"\"\"\n",
    "    all_text = []\n",
    "    all_wc = []\n",
    "    # Find all <TextLine> elements\n",
    "    for lines in xml.iterfind(\".//{%s}TextLine\" % xmlns):\n",
    "        # Find all <String> elements\n",
    "        for line in lines.findall(\"{%s}String\" % xmlns):\n",
    "            wc = line.attrib[\"WC\"]\n",
    "            if wc is not None:\n",
    "                all_wc.append(float(wc))\n",
    "            # Check if there are no hyphenated words\n",
    "            if \"SUBS_CONTENT\" not in line.attrib and \"SUBS_TYPE\" not in line.attrib:\n",
    "                # Get value of attribute @CONTENT from all <String> elements\n",
    "                text = line.attrib.get(\"CONTENT\")  # + ' '\n",
    "            elif \"HypPart1\" in line.attrib.get(\"SUBS_TYPE\"):\n",
    "                text = line.attrib.get(\"SUBS_CONTENT\")  # + ' '\n",
    "                if \"HypPart2\" in line.attrib.get(\"SUBS_TYPE\"):\n",
    "                    pass\n",
    "            all_text.append(text)\n",
    "    if all_wc:\n",
    "        mean_ocr = mean(all_wc)\n",
    "    if len(all_wc) > 2:\n",
    "        std_ocr = stdev(all_wc)\n",
    "    else:\n",
    "        mean_ocr = None\n",
    "        std_ocr = None\n",
    "    return \" \".join(all_text), mean_ocr, std_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, wc, std_ocr = get_alto_text(xml, ns)\n",
    "assert all([text, wc, std_ocr])\n",
    "assert isinstance(text, str)\n",
    "assert isinstance(wc, float)\n",
    "assert isinstance(std_ocr, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def alto_illustrations(xml, xmlns):\n",
    "    \"\"\"Extract bounding boxes of illustration from ALTO xml file\"\"\"\n",
    "    # Find all <Illustration> elements\n",
    "    bounding_boxes = []\n",
    "    for illustration in xml.iterfind(\".//{%s}Illustration\" % xmlns):\n",
    "        # Get @ID of <Illustration> element\n",
    "        illustration_id = illustration.attrib.get(\"ID\")\n",
    "        # Get coordinates of <Illustration> element\n",
    "        illustration_coords = list(\n",
    "            map(\n",
    "                float,\n",
    "                (\n",
    "                    illustration.attrib.get(\"HEIGHT\"),\n",
    "                    illustration.attrib.get(\"WIDTH\"),\n",
    "                    illustration.attrib.get(\"VPOS\"),\n",
    "                    illustration.attrib.get(\"HPOS\"),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        bounding_boxes.append(illustration_coords)\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alto_illustrations(xml, ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_illustrations(xmls):\n",
    "    for file in xmls:\n",
    "        with open(file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"Illustration\" in line:\n",
    "                    yield file\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustration_xmls = list(take(10, get_illustrations(alto_xmls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustration_xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in illustration_xmls:\n",
    "    fname, xml, ns = alto_parse(file)\n",
    "    bounding_boxes = alto_illustrations(xml, ns)\n",
    "    assert bounding_boxes\n",
    "    for box in bounding_boxes:\n",
    "        assert isinstance(box, list)\n",
    "        assert len(box) == 4\n",
    "        assert all(isinstance(x, float) for x in box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newspaper page container \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@define(slots=True)\n",
    "class NewspaperPageAlto:\n",
    "    fname: Union[str, Path]\n",
    "    text: Optional[str]\n",
    "    mean_ocr: Optional[float]\n",
    "    std_ocr: Optional[float]\n",
    "    bounding_boxes: List[Union[float, None]]\n",
    "    item_id: str = field(init=False)\n",
    "    def _get_id(self):\n",
    "        return \"/\".join(Path(self.fname).parts[-3:-1])\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.item_id = self._get_id()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def parse_newspaper_page(xml_fname: Union[str, Path]):\n",
    "    fname, xml, ns = alto_parse(xml_fname)\n",
    "    text, wc, std_ocr = get_alto_text(xml, ns)\n",
    "    bounding_boxes = alto_illustrations(xml, ns)\n",
    "    return NewspaperPageAlto(xml_fname, text, wc, std_ocr, bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = parse_newspaper_page(alto_xmls[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert page\n",
    "assert isinstance(page, NewspaperPageAlto)\n",
    "assert isinstance(page.text, (str, None))\n",
    "assert isinstance(page.mean_ocr, (float, None))\n",
    "assert isinstance(page.std_ocr, (float, None))\n",
    "assert isinstance(page.bounding_boxes, List)\n",
    "assert isinstance(page.item_id, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get metadata \n",
    "\n",
    "The next step is to create some functionality to get metadata for the items. There are two possible ways we can do this:\n",
    "- via the metadata download dumps\n",
    "- via the Europena API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_examples = list(take(500, Path(\"test_data\").rglob(\"*edm.xml\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@define(slots=True)\n",
    "class NewspaperPageMetadata:\n",
    "    metadata_xml_fname: Union[str, Path]\n",
    "    title: Optional[str]\n",
    "    date: Optional[str]\n",
    "    languages: Union[List[str], str, None]\n",
    "    item_iiif_url: Optional[str]\n",
    "    all_metadata_dict: Dict[Any, Any]\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.languages = (\n",
    "            self.languages.split(\",\")\n",
    "            if isinstance(self.languages, str)\n",
    "            else self.languages\n",
    "        )\n",
    "        self.title = self.title.split(\"-\")[0].strip(\" \")\n",
    "        self.metadata_xml_fname = str(self.metadata_xml_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_metadata_from_xml(xml_file: Union[Path, str]):\n",
    "    with open(xml_file, \"r\") as f:\n",
    "        xml = xmltodict.parse(f.read())\n",
    "    metadata = xml[\"rdf:RDF\"]\n",
    "    ProvidedCHO = metadata[\"edm:ProvidedCHO\"]\n",
    "    title = ProvidedCHO[\"dc:title\"]\n",
    "    data = ProvidedCHO[\"dcterms:issued\"]\n",
    "    languages = ProvidedCHO[\"dc:language\"]\n",
    "    iiif_url = metadata[\"ore:Aggregation\"][\"edm:isShownBy\"][\"@rdf:resource\"]\n",
    "    return NewspaperPageMetadata(xml_file, title, data, languages, iiif_url, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metadata_xml in metadata_examples:\n",
    "    metadata = get_metadata_from_xml(metadata_xml)\n",
    "    assert metadata\n",
    "    assert isinstance(metadata.languages, (list, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking metadata and XML\n",
    "We need to be able to link from the data we got from the ALTO XML with our metadata and smush them together. We have our `page.item_id` attribute which will hopefully be sufficient to grab the related metadata file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_metadata_for_page(\n",
    "    page: NewspaperPageAlto, metadata_directory: Optional[str] = None\n",
    "):\n",
    "    short_id = page.item_id.split(\"_\")[-1]\n",
    "    metadata_xml = f\"{metadata_directory}/http%3A%2F%2Fdata.theeuropeanlibrary.org%2FBibliographicResource%2F{short_id}.edm.xml\"\n",
    "    return get_metadata_from_xml(metadata_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_metadata_for_page(page, metadata_directory=\"test_data/metadata\")\n",
    "assert metadata\n",
    "assert isinstance(metadata, NewspaperPageMetadata)\n",
    "assert page.item_id.split(\"_\")[-1] in metadata.metadata_xml_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@define(slots=True)\n",
    "class NewspaperPage:\n",
    "    fname: Union[str, Path]\n",
    "    text: Optional[str]\n",
    "    mean_ocr: Optional[float]\n",
    "    std_ocr: Optional[float]\n",
    "    bounding_boxes: List[Union[float, None]]\n",
    "    item_id: str\n",
    "    metadata_xml_fname: Union[str, Path]\n",
    "    title: Optional[str]\n",
    "    date: Optional[str]\n",
    "    languages: Union[List[str], None]\n",
    "    item_iiif_url: Optional[str]\n",
    "    # all_metadata_dict: Dict[Any, Any]\n",
    "    multi_language: bool = field(init=False)\n",
    "    issue_uri: str = field(init=False)\n",
    "    id: str = field(init=False)\n",
    "    def __attrs_post_init__(self):\n",
    "        self.issue_uri = f\"https://www.europeana.eu/item/{self.item_id}\"\n",
    "        self.metadata_xml_fname = str(self.metadata_xml_fname)\n",
    "        self.languages = (\n",
    "            [lang for lang in self.languages if lang != \"==\"]\n",
    "            if isinstance(self.languages, list)\n",
    "            else self.languages\n",
    "        )\n",
    "        self.multi_language = (\n",
    "            isinstance(self.languages, list) and len(self.languages) > 1\n",
    "        )\n",
    "        self.id = f\"{self.issue_uri}/${self.fname.name.strip('.xml')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def process_newspaper_page(\n",
    "    xml_file: Union[str, Path], metadata_directory: Optional[str] = None\n",
    ") -> Dict[Any, Any]:\n",
    "    page = parse_newspaper_page(xml_file)\n",
    "    metadata = get_metadata_for_page(page, metadata_directory=metadata_directory)\n",
    "    metadata = asdict(metadata)\n",
    "    metadata.pop(\"all_metadata_dict\")\n",
    "    page = asdict(page)\n",
    "    return NewspaperPage(**page, **metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    process_newspaper_page(xml, metadata_directory=\"test_data/metadata\")\n",
    "    for xml in alto_xmls[:32]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = process_newspaper_page(alto_xmls[0], metadata_directory=\"test_data/metadata\")\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from datasets import Dataset\n",
    "from datasets import Value, Sequence, Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "features=Features({\n",
    "    'fname': Value(dtype='string', id=None),\n",
    "    'text': Value(dtype='string', id=None),\n",
    "    'mean_ocr': Value(dtype='float64', id=None),\n",
    "    'std_ocr': Value(dtype='float64', id=None),\n",
    "    'bounding_boxes': Sequence(\n",
    "        feature=Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None),\n",
    "        length=-1,\n",
    "        id=None\n",
    "    ),\n",
    "    'item_id': Value(dtype='string', id=None),\n",
    "    \"id\": Value(dtype=\"string\",id=None),\n",
    "    \"issue_uri\": Value(dtype=\"string\", id=None),\n",
    "    'metadata_xml_fname': Value(dtype='string', id=None),\n",
    "    'title': Value(dtype='string', id=None),\n",
    "    'date': Value(dtype='string', id=None),\n",
    "    'languages': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
    "    'item_iiif_url': Value(dtype='string', id=None),\n",
    "    'multi_language': Value(dtype='bool', id=None)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing a batch\n",
    "\n",
    "We create a function that will take a batch of XML files and load it into a dataset. We use `logger.catch` as a super lazy way of catching errors. `logger.catch` will catch exceptions and log so we can more easily debug and errors we run into as we work on this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_keys = {\"fname\",'item_id'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@logger.catch()\n",
    "def process_batch(xml_batch: Iterable[Union[str, Path]], metadata_directory: Optional[Union[str,Path]]=None)-> Dataset:\n",
    "    \"\"\"Returns a dataset containing parsed newspaper pages.\"\"\"\n",
    "    batch = [\n",
    "        asdict(process_newspaper_page(xml, metadata_directory=metadata_directory))\n",
    "        for xml in xml_batch\n",
    "    ]\n",
    "    batch = {key: [i[key] for i in batch] for key in batch[0]}\n",
    "    dataset = Dataset.from_dict(batch,features=features)\n",
    "    dataset = dataset.remove_columns([\"item_id\",\"metadata_xml_fname\",\"fname\"])\n",
    "    dataset = dataset.rename_columns({\"languages\":\"language\"})\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = process_batch(alto_xmls[:32], metadata_directory=\"test_data/metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = process_batch(alto_xmls[:32], metadata_directory=\"test_data/metadata\")\n",
    "assert len(ds) == 32\n",
    "assert len(ds.column_names) == 11\n",
    "assert ds.column_names == [\n",
    " 'text',\n",
    " 'mean_ocr',\n",
    " 'std_ocr',\n",
    " 'bounding_boxes',\n",
    " 'title',\n",
    " 'date',\n",
    " 'language',\n",
    " 'item_iiif_url',\n",
    " 'multi_language',\n",
    " 'issue_uri',\n",
    " 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import multiprocessing \n",
    "\n",
    "def process(\n",
    "    xml_files: Iterable[Union[str, Path]],\n",
    "    batch_size: int = 32,\n",
    "    metadata_directory: Optional[Union[str,Path]] = None,\n",
    "    max_workers: Optional[int] = None\n",
    "):\n",
    "    with tqdm(total=len(xml_files) // batch_size) as pbar:\n",
    "        if not max_workers:\n",
    "            max_workers = multiprocessing.cpu_count()\n",
    "        futures = []\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for batch in partition_all(batch_size, xml_files):\n",
    "                batch = list(batch)\n",
    "                future = executor.submit(\n",
    "                    process_batch, batch, metadata_directory=metadata_directory\n",
    "                )\n",
    "                future.add_done_callback(lambda p: pbar.update(1))\n",
    "                futures.append(future)\n",
    "    return [future.result() for future in as_completed(futures)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = process(alto_xmls, metadata_directory=\"test_data/metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets(datasets)\n",
    "dataset[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('europeana_alto')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
