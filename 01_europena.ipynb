{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp europena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Europeana newspaper parsers\n",
    "\n",
    "> Parsers for Europena newspapers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this code is to create a pipeline for parsing the [Europeana newspaper bulk downloads](https://pro.europeana.eu/page/iiif#download) and converting the orignal ALTO XML formats + metadata into a format that can be ingested easily into the ðŸ¤— [datasets](https://huggingface.co/docs/datasets/index) library and cons\n",
    "\n",
    " for #BigLAM. This code is mostly colated from other places. We used [nbdev](https://nbdev.fast.ai/) to give our code some:\n",
    "\n",
    "- basic tests\n",
    "- some basic documentation \n",
    "- make it easily instalable as a Python package. \n",
    "\n",
    "**note** some of these parsers are likely to be more generic but we'll develop them for europena newspapers for now. Once they have been tested on other collections they may be moved to a core module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import io\n",
    "import os\n",
    "import xml\n",
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from statistics import mean, stdev\n",
    "from attrs import asdict\n",
    "\n",
    "from typing import Any, Dict, Iterable, List, Optional, Union\n",
    "from attrs import define, field\n",
    "\n",
    "import xmltodict\n",
    "from toolz import partition_all\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALTO Processing\n",
    "\n",
    "ALTO is an XML format commonly used to store the outout of Opitcal Character Recogniton software. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test data\n",
    "\n",
    "A small amount of test data is included in the repository and versioned with Git. To avoid bloating the repository too much the below cell can either be run as part of test (including) slow tests or run locally to give oneself a bigger amount of data to test methods with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # |slow\n",
    "# !mkdir test_data\n",
    "# !aria2c -x 4 -d test_data/ ftp://download.europeana.eu/newspapers/fulltext/alto/9200396.zip\n",
    "# !unzip test_data/*.zip -d test_data/\n",
    "# !rm test_data/*.zip\n",
    "# !mkdir test_data/metadata\n",
    "# !aria2c -x 4 -d test_data/metadata/ ftp://download.europeana.eu/newspapers/metadata/9200396.zip\n",
    "# !unzip test_data/metadata/*.zip -d test_data/metadata/\n",
    "# !rm test_data/metadata/*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106562"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alto_xmls = [f for f in Path(\"test_data\").rglob(\"*.xml\") if \"edm\" not in f.name]\n",
    "len(alto_xmls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse ALTO XMLs\n",
    "\n",
    "The first step is to parse the xml file from disk into a elementree that we can use for other takss\n",
    "stolen from; https://github.com/cneud/alto-tools/blob/master/alto_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def alto_parse(alto: Union[str, Path], **kwargs):\n",
    "    \"\"\"Convert ALTO xml file to element tree\"\"\"\n",
    "    try:\n",
    "        xml = ET.parse(alto, **kwargs)\n",
    "    except ET.ParseError as e:\n",
    "        logger.error(f\"Parser Error in file '{alto}': {e}\")\n",
    "        return None\n",
    "    # Register ALTO namespaces\n",
    "    # https://www.loc.gov/standards/alto/ | https://github.com/altoxml\n",
    "    # alto-bnf (unoffical) BnF ALTO dialect - for further info see\n",
    "    # http://bibnum.bnf.fr/alto_prod/documentation/alto_prod.html\n",
    "    namespace = {\n",
    "        \"alto-1\": \"http://schema.ccs-gmbh.com/ALTO\",\n",
    "        \"alto-2\": \"http://www.loc.gov/standards/alto/ns-v2#\",\n",
    "        \"alto-3\": \"http://www.loc.gov/standards/alto/ns-v3#\",\n",
    "        \"alto-4\": \"http://www.loc.gov/standards/alto/ns-v4#\",\n",
    "        \"alto-5\": \"http://schema.ccs-gmbh.com/docworks/version20/alto-1-4.xsd\",\n",
    "        \"alto-bnf\": \"http://bibnum.bnf.fr/ns/alto_prod\",\n",
    "    }\n",
    "    # Extract namespace from document root\n",
    "    if \"http://\" in str(xml.getroot().tag.split(\"}\")[0].strip(\"{\")):\n",
    "        xmlns = xml.getroot().tag.split(\"}\")[0].strip(\"{\")\n",
    "    else:\n",
    "        try:\n",
    "            ns = xml.getroot().attrib\n",
    "            xmlns = str(ns).split(\" \")[1].strip(\"}\").strip(\"'\")\n",
    "        except IndexError:\n",
    "            logger.warning(f\"File {alto.name}: no namespace declaration found.\")\n",
    "            xmlns = \"no_namespace_found\"\n",
    "    if xmlns in namespace.values():\n",
    "        return alto, xml, xmlns\n",
    "    else:\n",
    "        logger.warning(f\"File {alto.name}: namespace {xmlns} is not registered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('test_data/9200396/BibliographicResource_3000118436002/88.xml')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alto_xmls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, xml, ns = alto_parse(alto_xmls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 12:11:06.762 | ERROR    | __main__:alto_parse:7 - Parser Error in file 'fake.xml': no element found: line 1, column 0\n"
     ]
    }
   ],
   "source": [
    "assert all([fname, xml, ns])\n",
    "Path(\"fake.xml\").touch(exist_ok=True)\n",
    "bad_xml = alto_parse(Path(\"fake.xml\"))\n",
    "# assert isinstance(bad_xml, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_alto_text(xml, xmlns, join_lines=True):\n",
    "    \"\"\"Extract text content from ALTO xml file\"\"\"\n",
    "    all_text = []\n",
    "    all_wc = []\n",
    "    # Find all <TextLine> elements\n",
    "    for lines in xml.iterfind(\".//{%s}TextLine\" % xmlns):\n",
    "        # Find all <String> elements\n",
    "        for line in lines.findall(\"{%s}String\" % xmlns):\n",
    "            wc = line.attrib[\"WC\"]\n",
    "            if wc is not None:\n",
    "                all_wc.append(float(wc))\n",
    "            # Check if there are no hyphenated words\n",
    "            if \"SUBS_CONTENT\" not in line.attrib and \"SUBS_TYPE\" not in line.attrib:\n",
    "                # Get value of attribute @CONTENT from all <String> elements\n",
    "                text = line.attrib.get(\"CONTENT\")  # + ' '\n",
    "            elif \"HypPart1\" in line.attrib.get(\"SUBS_TYPE\"):\n",
    "                text = line.attrib.get(\"SUBS_CONTENT\")  # + ' '\n",
    "                if \"HypPart2\" in line.attrib.get(\"SUBS_TYPE\"):\n",
    "                    pass\n",
    "            all_text.append(text)\n",
    "    if all_wc:\n",
    "        mean_ocr = mean(all_wc)\n",
    "    if len(all_wc) > 2:\n",
    "        std_ocr = stdev(all_wc)\n",
    "    else:\n",
    "        mean_ocr = None\n",
    "        std_ocr = None\n",
    "    return \" \".join(all_text), mean_ocr, std_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, wc, std_ocr = get_alto_text(xml, ns)\n",
    "assert all([text, wc, std_ocr])\n",
    "assert isinstance(text, str)\n",
    "assert isinstance(wc, float)\n",
    "assert isinstance(std_ocr, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def alto_illustrations(xml, xmlns):\n",
    "    \"\"\"Extract bounding boxes of illustration from ALTO xml file\"\"\"\n",
    "    # Find all <Illustration> elements\n",
    "    bounding_boxes = []\n",
    "    for illustration in xml.iterfind(\".//{%s}Illustration\" % xmlns):\n",
    "        # Get @ID of <Illustration> element\n",
    "        illustration_id = illustration.attrib.get(\"ID\")\n",
    "        # Get coordinates of <Illustration> element\n",
    "        illustration_coords = list(\n",
    "            map(\n",
    "                float,\n",
    "                (\n",
    "                    illustration.attrib.get(\"HEIGHT\"),\n",
    "                    illustration.attrib.get(\"WIDTH\"),\n",
    "                    illustration.attrib.get(\"VPOS\"),\n",
    "                    illustration.attrib.get(\"HPOS\"),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        bounding_boxes.append(illustration_coords)\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alto_illustrations(xml, ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_illustrations(xmls):\n",
    "    for file in xmls:\n",
    "        with open(file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"Illustration\" in line:\n",
    "                    yield file\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustration_xmls = list(take(10, get_illustrations(alto_xmls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('test_data/9200396/BibliographicResource_3000118436002/1.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118435687/76.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118435687/3.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118435687/1.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118436230/76.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118436230/9.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118436230/20.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118436230/33.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118436230/32.xml'),\n",
       " Path('test_data/9200396/BibliographicResource_3000118436230/1.xml')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "illustration_xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in illustration_xmls:\n",
    "    fname, xml, ns = alto_parse(file)\n",
    "    bounding_boxes = alto_illustrations(xml, ns)\n",
    "    assert bounding_boxes\n",
    "    for box in bounding_boxes:\n",
    "        assert isinstance(box, list)\n",
    "        assert len(box) == 4\n",
    "        assert all(isinstance(x, float) for x in box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newspaper page container \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@define(slots=True)\n",
    "class NewspaperPageAlto:\n",
    "    fname: Union[str, Path]\n",
    "    text: Optional[str]\n",
    "    mean_ocr: Optional[float]\n",
    "    std_ocr: Optional[float]\n",
    "    bounding_boxes: List[Union[float, None]]\n",
    "    item_id: str = field(init=False)\n",
    "\n",
    "    def _get_id(self):\n",
    "        return \"/\".join(Path(self.fname).parts[-3:-1])\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.item_id = self._get_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def parse_newspaper_page(xml_fname: Union[str, Path]):\n",
    "    fname, xml, ns = alto_parse(xml_fname)\n",
    "    text, wc, std_ocr = get_alto_text(xml, ns)\n",
    "    bounding_boxes = alto_illustrations(xml, ns)\n",
    "    return NewspaperPageAlto(xml_fname, text, wc, std_ocr, bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = parse_newspaper_page(alto_xmls[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert page\n",
    "assert isinstance(page, NewspaperPageAlto)\n",
    "assert isinstance(page.text, (str, None))\n",
    "assert isinstance(page.mean_ocr, (float, None))\n",
    "assert isinstance(page.std_ocr, (float, None))\n",
    "assert isinstance(page.bounding_boxes, List)\n",
    "assert isinstance(page.item_id, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewspaperPageAlto(fname=Path('test_data/9200396/BibliographicResource_3000118436002/9.xml'), text=\"Ã¯g. HÃ©ctmlre . SSS du thrÃ´ne & de lÃ©giflation dans le roÃ¯aume, lâ€™extinÃ¢ion des guerres entre le Souverain & les vaffaux , entre les provinces & les provinces provinces ; quâ€™ainfi fans entrer dans les motifs Ã  ne juger que par lâ€™Ã©vÃ©nementles croifa- des ont Ã©pargnÃ© Ã  la nation plus de fang quâ€™elles nâ€™en ont fait rÃ©pandre, quâ€™elles ont pofÃ© la bafe & les fondemens de la tranquillitÃ© tranquillitÃ© intÃ©rieure de lâ€™Ã©tat. Je ne vous ferai pas remarquer que lâ€™entreprife de Louis nâ€™offroit point dâ€™obftacles capables de ralentir fort ^ .courage & dâ€™alarmer fa fagefl'e; quâ€™on a voit vÃ» Godefroy de Bouillon , avec les dÃ©bris dâ€™une armÃ©e, amas fortuit de diverfes nations, nations, Ã©lever dans JÃ©rufalem un thrÃ´ne qui, vainqueur des Mufulmans , nâ€™a pÃ©ri que par Autres les di vidons & les perfidies des chrÃ©tiens, je rÃ©fl. fur les nâ€™ajouterai point que Chypre, Conftantino- croifadesii pie, fournis Ã  des princes de la communion A Q Q a lÃ‚oÃ»ÃŠ latine, donnoit Ã  Louis des facilitÃ©s que nâ€™eut , 77 g, p .. point Godefroid de Bouillon â€ž. â€” 15. DÃ©c, Le parallÃ¨le de Bajazet & de St, Louis, P- quâ€™on lit dans le panÃ©gyrique de ce Saint, eft un tableau dont tous les traits concou rent au triomphe de la religion du faint Roi. Les . panÃ©gyriftes des hÃ©ros turcs & Chinois, ( & i! y en a aujourdâ€™hui un bien grand nombre ) font priÃ©s dâ€™en faire un qui puiffe lÃ©rvir de pendant Ã  celui-ci. â€œ Que le fe&ateur de Mahomet dÃ©ploie maintenant fes faites, il nous montrera lâ€™Aile , lâ€™Afrique, lâ€™Afrique, lâ€™Europe englouties par fes conquÃ©rons , & le thrÃ´ne de fes Califes Ã©levÃ© fur les ruines fie lâ€™apivers j il nous piontrera donc ce courage courage\", mean_ocr=0.5861000468510345, std_ocr=0.1977007715324908, bounding_boxes=[], item_id='9200396/BibliographicResource_3000118436002')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get metadata \n",
    "\n",
    "The next step is to create some functionality to get metadata for the items. There are two possible ways we can do this:\n",
    "- via the metadata download dumps\n",
    "- via the Europena API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_examples = list(take(500, Path(\"test_data\").rglob(\"*edm.xml\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@define(slots=True)\n",
    "class NewspaperPageMetadata:\n",
    "    metadata_xml_fname: Union[str, Path]\n",
    "    title: Optional[str]\n",
    "    issued: Optional[str]\n",
    "    language: Union[List[str], str, None]\n",
    "    item_iiif_url: Optional[str]\n",
    "    all_metadata_dict: Dict[Any, Any]\n",
    "    issued_parsed: datetime = field(init=False)\n",
    "\n",
    "    def _parse_date(self, issued):\n",
    "        try:\n",
    "            date = datetime.strptime(issued, \"%Y-%m-%d\")\n",
    "            return date\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.language = (\n",
    "            self.language.split(\",\")\n",
    "            if isinstance(self.language, str)\n",
    "            else self.language\n",
    "        )\n",
    "\n",
    "        self.title = self.title.split(\"-\")[0].strip(\" \")\n",
    "        self.metadata_xml_fname = str(self.metadata_xml_fname)\n",
    "        self.issued_parsed = self._parse_date(self.issued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_metadata_from_xml(xml_file: Union[Path, str]):\n",
    "    with open(xml_file, \"r\") as f:\n",
    "        xml = xmltodict.parse(f.read())\n",
    "    metadata = xml[\"rdf:RDF\"]\n",
    "    ProvidedCHO = metadata[\"edm:ProvidedCHO\"]\n",
    "    title = ProvidedCHO[\"dc:title\"]\n",
    "    issued = ProvidedCHO[\"dcterms:issued\"]\n",
    "    language = ProvidedCHO[\"dc:language\"]\n",
    "    iiif_url = metadata[\"ore:Aggregation\"][\"edm:isShownBy\"][\"@rdf:resource\"]\n",
    "    return NewspaperPageMetadata(xml_file, title, issued, language, iiif_url, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = metadata_examples[-23]\n",
    "with open(xml_file, \"r\") as f:\n",
    "    xml = xmltodict.parse(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metadata_xml in metadata_examples:\n",
    "    metadata = get_metadata_from_xml(metadata_xml)\n",
    "    assert metadata\n",
    "    assert isinstance(metadata.language, (list, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking metadata and XML\n",
    "We need to be able to link from the data we got from the ALTO XML with our metadata and smush them together. We have our `page.item_id` attribute which will hopefully be sufficient to grab the related metadata file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1783, 6, 1, 0, 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.issued_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_metadata_for_page(\n",
    "    page: NewspaperPageAlto, metadata_directory: Optional[str] = None\n",
    "):\n",
    "    short_id = page.item_id.split(\"_\")[-1]\n",
    "    metadata_xml = f\"{metadata_directory}/http%3A%2F%2Fdata.theeuropeanlibrary.org%2FBibliographicResource%2F{short_id}.edm.xml\"\n",
    "    return get_metadata_from_xml(metadata_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_metadata_for_page(page, metadata_directory=\"test_data/metadata\")\n",
    "assert metadata\n",
    "assert isinstance(metadata, NewspaperPageMetadata)\n",
    "assert page.item_id.split(\"_\")[-1] in metadata.metadata_xml_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@define(slots=True)\n",
    "class NewspaperPage:\n",
    "    fname: Union[str, Path]\n",
    "    text: Optional[str]\n",
    "    mean_ocr: Optional[float]\n",
    "    std_ocr: Optional[float]\n",
    "    bounding_boxes: List[Union[float, None]]\n",
    "    item_id: str\n",
    "    metadata_xml_fname: Union[str, Path]\n",
    "    title: Optional[str]\n",
    "    issued: Optional[str]\n",
    "    issued_parsed: Optional[datetime]\n",
    "    language: Union[List[str], None]\n",
    "    item_iiif_url: Optional[str]\n",
    "    # all_metadata_dict: Dict[Any, Any]\n",
    "    multi_language: bool = field(init=False)\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self.fname = str(self.fname)\n",
    "        self.metadata_xml_fname = str(self.metadata_xml_fname)\n",
    "        self.language = (\n",
    "            [lang for lang in self.language if lang != \"==\"]\n",
    "            if isinstance(self.language, list)\n",
    "            else self.language\n",
    "        )\n",
    "        self.multi_language = isinstance(self.language, list) and len(self.language) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def process_newspaper_page(\n",
    "    xml_file: Union[str, Path], metadata_directory: Optional[str] = None\n",
    ") -> Dict[Any, Any]:\n",
    "    page = parse_newspaper_page(xml_file)\n",
    "    metadata = get_metadata_for_page(page, metadata_directory=metadata_directory)\n",
    "    metadata = asdict(metadata)\n",
    "    metadata.pop(\"all_metadata_dict\")\n",
    "    page = asdict(page)\n",
    "    return NewspaperPage(**page, **metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    process_newspaper_page(xml, metadata_directory=\"test_data/metadata\")\n",
    "    for xml in alto_xmls[:32]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewspaperPage(fname='test_data/9200396/BibliographicResource_3000118436002/88.xml', text='Idem , par Mr***. en lui prifentent un exemplaire de fes fermons imprimÃ©s. 15. DÃ©cembre. DÃ©cembre. Voix {_ la), du vi ni patriote catholique, oppofÃ©e Ã  celle des faux patriotes toltrans. 1. Septembre. 1 1 â– ladre parmi les Ombres, avec cette Ã©pigraphe: etgo erruvtmus & errare fecimus. 1. Novembre. . . 332 Voltaire de retour des Ombres , & fur le point dâ€™y retourner pour nâ€™en plus revenir l. Novembre. Page 57o 333 â€™On fait favoir que les biens des ci devant JÃ©fuites de la niÃ©taiÃ®ie Ã  Kockellchcur Ã  Â«ne lieue de cette ville, confftans ea maifocs, terres, prairies , Ã©tangs & bois , partagÃ©s en dix-neuf lots , font Ã  vendre , chaque lot fÃ©parÃ©- ment ; que les amateurs dâ€™un ou plulÃ®eurs lots pourront sâ€™adrefler au Receveur des Domaines Luxembourg Leonardy pendant le mois de Janvier prochain , & faire leurs offres , aprÃ¨s lequel mois Ã©coulÃ© lâ€™on ne recevera plus aucune offie i & que le 19e. de ce mois de DÃ©cembre, a s heures du matin & jours fuivans, le dit Receveur vendra au dit Kockclfcheur tous les dirvaux foin, paille, &. les meubles de ladite mÃ©tairie .au plus offrant & dernier enchÃ©rilleur, argent comptant. Luxembourg le ; DÃ©cembre 1773. CD bÃªtes Ã  cornes & autres beftianx >', mean_ocr=0.44245891427834905, std_ocr=0.18921927369326655, bounding_boxes=[], item_id='9200396/BibliographicResource_3000118436002', metadata_xml_fname='test_data/metadata/http%3A%2F%2Fdata.theeuropeanlibrary.org%2FBibliographicResource%2F3000118436002.edm.xml', title='Journal historique et littÃ©raire', issued='1776-12-15', issued_parsed=datetime.datetime(1776, 12, 15, 0, 0), language=['fr'], item_iiif_url='https://iiif.europeana.eu/image/2TS6TSUK5ULAT2TQMYN7UGBDCPKQBLHQPTPDD6GGYB4QOZXR72EQ/presentation_images/bc994340-0232-11e6-a696-fa163e2dd531/node-3/image/BNL/Journal_historique_et_littÃ©raire/1776/12/15/00547/full/full/0/default.jpg', multi_language=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_newspaper_page(alto_xmls[0], metadata_directory=\"test_data/metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewspaperPage(fname='test_data/9200396/BibliographicResource_3000118436230/38.xml', text=\"Journal hÃ®fl. & litt . licjri , rÃ©gent de Lithuanie, â€”â€” regimens dâ€™infanterie, qui font au nombre de 38 , doivent Ãªtre portÃ©s Ã  j 500 hommes, hommes, au lieu de 1200, dont ils Ã©toient Compofcs ci'- devant ; lÃ©s pontonniers qui Ã©toient de 608 hommes, en formeront 1000, ainfi que le corps dâ€™artillerie ; pour cÃ§ dernier on nâ€™admet que des gens inftruits & qui ont achevÃ© leurs Ã©tudes. La cavalerie- nationale sâ€™accroÃ®t de jour en jour. Chaque rÃ©giment dâ€™Oulans fera augmentÃ© de 290 hommes. hommes. 'On entend par-tout les tambours annoncer annoncer la recrue, depuis environ 4 femaines, il nous eft arrive au-delÃ  de 5000 des plus beaux hommes, la plupart dÃ©ferteurs allemands. Jufquâ€™ici nous' ignorons abfolument lâ€™objet dâ€™une levÃ©e fi nombrenfe & fi. extraordinaire. Dantzig (/e 3 Janvier. ) Lâ€™aflÃ¨mblÃ©e des trois Ordres de la ville , convoquÃ©e le c8 DÃ©cembre, en confÃ©quence des ordres venus de Varfovie pour figner la convention relative relative Ã  la navigation de la Viftule, fut prorogÃ©e prorogÃ©e au lendemain ; & câ€™eft ce jour-lÃ  que les trois Ordres ont donnÃ© leur confentement unanime pour la fignature de cette convention. convention. Le magiftrat a fait informer de cette, rÃ©folution M r . de Pfennig, commiffaire de Pologne, M r . de PÃ©terfon , rÃ©fident de Ruf- fie, & M r . de Lindenowski, rÃ©fident de Pruffe, avec priere dâ€™en rendre compte Ã  leurs cours refpeÃ¢ives. Cependant le peuple en gÃ©nÃ©ral gÃ©nÃ©ral nâ€™eft pas content, & on craint que dans lâ€™occafion il nâ€™cclate en murmures & peut- %re en tumultes fÃ©ditieux. Tous nos\", mean_ocr=0.5256414447269231, std_ocr=0.19060038635865742, bounding_boxes=[], item_id='9200396/BibliographicResource_3000118436230', metadata_xml_fname='test_data/metadata/http%3A%2F%2Fdata.theeuropeanlibrary.org%2FBibliographicResource%2F3000118436230.edm.xml', title='Journal historique et littÃ©raire', issued='1785-02-01', issued_parsed=datetime.datetime(1785, 2, 1, 0, 0), language=['fr'], item_iiif_url='https://iiif.europeana.eu/image/NGJNELURKNAPK5EL35D32X3AV36I7Z2X67NW4OGTN3EEO7L5N6NA/presentation_images/b171fd00-0222-11e6-a696-fa163e2dd531/node-3/image/BNL/Journal_historique_et_littÃ©raire/1785/02/01/00157/full/full/0/default.jpg', multi_language=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = process_newspaper_page(alto_xmls[200], metadata_directory=\"test_data/metadata\")\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from datasets import Dataset\n",
    "from datasets import Value, Sequence, Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "features = Features(\n",
    "    {\n",
    "        \"fname\": Value(dtype=\"string\", id=None),\n",
    "        \"text\": Value(dtype=\"string\", id=None),\n",
    "        \"mean_ocr\": Value(dtype=\"float64\", id=None),\n",
    "        \"std_ocr\": Value(dtype=\"float64\", id=None),\n",
    "        \"bounding_boxes\": Sequence(\n",
    "            feature=Sequence(\n",
    "                feature=Value(dtype=\"float64\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            length=-1,\n",
    "            id=None,\n",
    "        ),\n",
    "        \"item_id\": Value(dtype=\"string\", id=None),\n",
    "        \"metadata_xml_fname\": Value(dtype=\"string\", id=None),\n",
    "        \"title\": Value(dtype=\"string\", id=None),\n",
    "        \"issued\": Value(dtype=\"string\", id=None),\n",
    "        \"issued_parsed\": Value(dtype=\"date64\", id=None),\n",
    "        \"language\": Sequence(\n",
    "            feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "        ),\n",
    "        \"item_iiif_url\": Value(dtype=\"string\", id=None),\n",
    "        \"multi_language\": Value(dtype=\"bool\", id=None),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@logger.catch()\n",
    "def process_batch(xml_batch: Iterable[Union[str, Path]], metadata_directory=None):\n",
    "    batch = [\n",
    "        asdict(process_newspaper_page(xml, metadata_directory=metadata_directory))\n",
    "        for xml in xml_batch\n",
    "    ]\n",
    "\n",
    "    batch = {key: [i[key] for i in batch] for key in batch[0]}\n",
    "\n",
    "    return Dataset.from_dict(batch, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = process_batch(alto_xmls[:32], metadata_directory=\"test_data/metadata\")\n",
    "assert len(ds) == 32\n",
    "assert len(ds.column_names) == 13\n",
    "assert ds.column_names == [\n",
    "    \"fname\",\n",
    "    \"text\",\n",
    "    \"mean_ocr\",\n",
    "    \"std_ocr\",\n",
    "    \"bounding_boxes\",\n",
    "    \"item_id\",\n",
    "    \"metadata_xml_fname\",\n",
    "    \"title\",\n",
    "    \"issued\",\n",
    "    \"issued_parsed\",\n",
    "    \"language\",\n",
    "    \"item_iiif_url\",\n",
    "    \"multi_language\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fname', 'text', 'mean_ocr', 'std_ocr', 'bounding_boxes', 'item_id', 'metadata_xml_fname', 'title', 'issued', 'issued_parsed', 'language', 'item_iiif_url', 'multi_language'],\n",
       "    num_rows: 32\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fname': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'mean_ocr': Value(dtype='float64', id=None),\n",
       " 'std_ocr': Value(dtype='float64', id=None),\n",
       " 'bounding_boxes': Sequence(feature=Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'item_id': Value(dtype='string', id=None),\n",
       " 'metadata_xml_fname': Value(dtype='string', id=None),\n",
       " 'title': Value(dtype='string', id=None),\n",
       " 'issued': Value(dtype='string', id=None),\n",
       " 'issued_parsed': Value(dtype='date64', id=None),\n",
       " 'language': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'item_iiif_url': Value(dtype='string', id=None),\n",
       " 'multi_language': Value(dtype='bool', id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "def process(\n",
    "    xml_files: Iterable[Union[str, Path]],\n",
    "    batch_size: int = 32,\n",
    "    metadata_directory: Optional[Union[str, Path]] = None,\n",
    "    max_workers: Optional[int] = None,\n",
    "):\n",
    "    with tqdm(total=len(xml_files) // batch_size) as pbar:\n",
    "        if not max_workers:\n",
    "            max_workers = multiprocessing.cpu_count()\n",
    "        futures = []\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for batch in partition_all(batch_size, xml_files):\n",
    "                batch = list(batch)\n",
    "                future = executor.submit(\n",
    "                    process_batch, batch, metadata_directory=metadata_directory\n",
    "                )\n",
    "                future.add_done_callback(lambda p: pbar.update(1))\n",
    "                futures.append(future)\n",
    "    return [future.result() for future in as_completed(futures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d85da29ae2d43c580b96a73ae95c20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = process(alto_xmls[:10], metadata_directory=\"test_data/metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fname', 'text', 'mean_ocr', 'std_ocr', 'bounding_boxes', 'item_id', 'metadata_xml_fname', 'title', 'issued', 'issued_parsed', 'language', 'item_iiif_url', 'multi_language'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = concatenate_datasets(datasets)\n",
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
